{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVwv5hdwM0hW","executionInfo":{"status":"ok","timestamp":1682186296589,"user_tz":-120,"elapsed":1911,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"outputId":"e613ccb3-357e-4f8c-b86e-c6969143eb22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Vamos a ver un ejemplo con un conjunto de opiniones (positivas o negativas) de restaurantes del recomendador Yelp\n","\n","import pandas as pd\n","\n","yelp_file = '/content/drive/MyDrive/IA2/p3/yelp_labelled.txt'\n","\n","df=pd.read_csv(yelp_file, delimiter='\\t', names=['sentence', 'sentiment'])\n","\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"XRlgg-amkZ2Z","executionInfo":{"status":"ok","timestamp":1682186296590,"user_tz":-120,"elapsed":34,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"outputId":"c0eb30e8-34fe-4c67-ed62-0f40eea52e49"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              sentence  sentiment\n","0                             Wow... Loved this place.          1\n","1                                   Crust is not good.          0\n","2            Not tasty and the texture was just nasty.          0\n","3    Stopped by during the late May bank holiday of...          1\n","4    The selection on the menu was great and so wer...          1\n","..                                                 ...        ...\n","995  I think food should have flavor and texture an...          0\n","996                           Appetite instantly gone.          0\n","997  Overall I was not impressed and would not go b...          0\n","998  The whole experience was underwhelming, and I ...          0\n","999  Then, as if I hadn't wasted enough of my life ...          0\n","\n","[1000 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-b0065ec2-ce84-4f06-a6d9-ab07517a116b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Wow... Loved this place.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Crust is not good.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Not tasty and the texture was just nasty.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Stopped by during the late May bank holiday of...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The selection on the menu was great and so wer...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>I think food should have flavor and texture an...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>Appetite instantly gone.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>Overall I was not impressed and would not go b...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>The whole experience was underwhelming, and I ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>Then, as if I hadn't wasted enough of my life ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0065ec2-ce84-4f06-a6d9-ab07517a116b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b0065ec2-ce84-4f06-a6d9-ab07517a116b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b0065ec2-ce84-4f06-a6d9-ab07517a116b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"8WM2LCP-5HXf"},"source":["# Usando word embeddings\n","\n","Este notebook está basado en el notebook del segundo ejemplo del Capítulo 6, Sección 1 de [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). \n","\n","[Aquí hay una versión gratuita del capítulo](https://freecontent.manning.com/deep-learning-for-text/)\n","\n","---\n"]},{"cell_type":"markdown","source":["Las word-embeddings son representaciones de palabras en espacios de varias dimensiones en los que cada palabra del diccionario es representada mediante un vector de varias dimensiones (decenas o centenas) que permiten capturar su relación con otras palabras\n","\n","Existen distintas técnicas para estimar word-embeddings a partir de un corpus de documentos y fijando el número de dimensiones que queremos usar.\n","\n","El resultado es una matriz de tantas columnas como palabras tenga el diccionario y tantas filas como dimensiones hayamos determinado.\n"],"metadata":{"id":"NbZkDrBnx9Ft"}},{"cell_type":"markdown","metadata":{"id":"4YAhPMNh5HXj"},"source":["There are two ways to obtain word embeddings:\n","\n","* Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). \n","In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network.\n","* Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. \n","These are called \"pre-trained word embeddings\". \n","\n","Let's take a look at both."]},{"cell_type":"markdown","metadata":{"id":"Y_tTqd1k5HXk"},"source":["## Learning word embeddings with the `Embedding` layer\n","\n","\n","Is there some \"ideal\" word embedding space that would perfectly map human language and could be used for any natural language processing \n","task? Possibly, but in any case, we have yet to compute anything of the sort. Also, there isn't such a thing as \"human language\", there are \n","many different languages and they are not isomorphic, as a language is the reflection of a specific culture and a specific context. But more \n","pragmatically, what makes a good word embedding space depends heavily on your task: the perfect word embedding space for an \n","English-language movie review sentiment analysis model may look very different from the perfect embedding space for an English-language \n","legal document classification model, because the importance of certain semantic relationships varies from task to task.\n","\n","It is thus reasonable to __learn__ a new embedding space with every new task. Thankfully, backpropagation makes this really easy, and Keras makes it \n","even easier. It's just about learning the weights of a layer: the `Embedding` layer."]},{"cell_type":"markdown","metadata":{"id":"YTAx1GYl5HXm"},"source":["\n","The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes \n","as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup."]},{"cell_type":"markdown","metadata":{"id":"ijS5gE_Z5HXm"},"source":["\n","The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of \n","integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have \n","shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must \n","have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded \n","with zeros, and sequences that are longer should be truncated.\n","\n","When you instantiate an `Embedding` layer, its weights (its internal dictionary of token vectors) are initially random, just like with any \n","other layer. During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the \n","downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for \n","the specific problem you were training your model for.\n"]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","\n","max_words = 1500\n","max_comment_length = 20\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(df.sentence)\n","\n","sequences = tokenizer.texts_to_sequences(df.sentence)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","max_words = len(word_index)\n","\n","data = pad_sequences(sequences, maxlen=max_comment_length)"],"metadata":{"id":"jWi6XDaxBAom","executionInfo":{"status":"ok","timestamp":1682186296590,"user_tz":-120,"elapsed":28,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aab7a88e-2553-497e-aba7-6db16a10e0f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2071 unique tokens.\n"]}]},{"cell_type":"code","source":["print(df.sentence[4])\n","print(data[4])\n","print(df.sentiment[4])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccvxgdjZ0NwO","executionInfo":{"status":"ok","timestamp":1682186296590,"user_tz":-120,"elapsed":23,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"outputId":"e0ba4ade-060c-42a6-f12b-d9b20b2af680"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The selection on the menu was great and so were the prices.\n","[  0   0   0   0   0   0   0   0   1 166  35   1 109   4  22   2  26  27\n","   1 167]\n","1\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","#valor de random state\n","rs=333\n","\n","d=df.values\n","\n","x_train, x_test, y_train, y_test = train_test_split(data, df.sentiment, test_size=0.20, random_state=rs, stratify = df.sentiment)\n","\n","print(\"Training texts:\", len(y_train))\n","print(\"Test texts:\", len(y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHUP_sBf9IQt","executionInfo":{"status":"ok","timestamp":1682186296591,"user_tz":-120,"elapsed":21,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"outputId":"c86915c8-c572-4cfe-c1fe-01d33fef6028"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training texts: 800\n","Test texts: 200\n"]}]},{"cell_type":"code","source":["# Fijamos el tamaño de los embedding a 50 dimensiones\n","\n","embedding_dim = 50"],"metadata":{"id":"6YL5tXWuu1v6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2P1FmfpU5HXn","executionInfo":{"status":"ok","timestamp":1682186299616,"user_tz":-120,"elapsed":3038,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"33f38c2b-cd45-4f8d-ce78-d13775d8fdc6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_3 (Embedding)     (None, 20, 50)            103550    \n","                                                                 \n"," flatten_3 (Flatten)         (None, 1000)              0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 1001      \n","                                                                 \n","=================================================================\n","Total params: 104,551\n","Trainable params: 104,551\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/20\n","25/25 [==============================] - 1s 8ms/step - loss: 0.6916 - accuracy: 0.5150 - val_loss: 0.6914 - val_accuracy: 0.5500\n","Epoch 2/20\n","25/25 [==============================] - 0s 5ms/step - loss: 0.6674 - accuracy: 0.7325 - val_loss: 0.6885 - val_accuracy: 0.5550\n","Epoch 3/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6384 - accuracy: 0.7925 - val_loss: 0.6830 - val_accuracy: 0.5650\n","Epoch 4/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.9262 - val_loss: 0.6750 - val_accuracy: 0.6300\n","Epoch 5/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.5385 - accuracy: 0.9337 - val_loss: 0.6634 - val_accuracy: 0.6450\n","Epoch 6/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.4670 - accuracy: 0.9563 - val_loss: 0.6499 - val_accuracy: 0.6750\n","Epoch 7/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.9688 - val_loss: 0.6354 - val_accuracy: 0.6700\n","Epoch 8/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.9812 - val_loss: 0.6241 - val_accuracy: 0.6800\n","Epoch 9/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.2662 - accuracy: 0.9825 - val_loss: 0.6132 - val_accuracy: 0.6850\n","Epoch 10/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.9862 - val_loss: 0.6066 - val_accuracy: 0.6850\n","Epoch 11/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.1794 - accuracy: 0.9912 - val_loss: 0.5985 - val_accuracy: 0.6850\n","Epoch 12/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.1488 - accuracy: 0.9925 - val_loss: 0.5956 - val_accuracy: 0.6800\n","Epoch 13/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.1244 - accuracy: 0.9937 - val_loss: 0.5921 - val_accuracy: 0.6750\n","Epoch 14/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.1051 - accuracy: 0.9975 - val_loss: 0.5916 - val_accuracy: 0.6800\n","Epoch 15/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.9975 - val_loss: 0.5917 - val_accuracy: 0.6850\n","Epoch 16/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.0769 - accuracy: 1.0000 - val_loss: 0.5915 - val_accuracy: 0.6900\n","Epoch 17/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 1.0000 - val_loss: 0.5924 - val_accuracy: 0.6950\n","Epoch 18/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.0584 - accuracy: 1.0000 - val_loss: 0.5951 - val_accuracy: 0.6850\n","Epoch 19/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 0.5992 - val_accuracy: 0.6900\n","Epoch 20/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 0.6018 - val_accuracy: 0.6900\n","7/7 [==============================] - 0s 2ms/step - loss: 0.6018 - accuracy: 0.6900\n","Accuracy: 69.00%\n"]}],"source":["# MODELO 1. SIN EMBEDDINGS PRE-ENTRENADOS \n","\n","from keras.models import Sequential\n","from keras.layers import Flatten, Dense, Embedding\n","\n","model1 = Sequential()\n","# We specify the maximum input length to our Embedding layer\n","# so we can later flatten the embedded inputs\n","\n","\n","model1.add(Embedding(max_words, embedding_dim, input_length=max_comment_length))\n","# After the Embedding layer, our activations have shape `(max_words, max_comment_length, embedding_dim)`.\n","\n","# We flatten the 3D tensor of embeddings into a 2D tensor of shape `(max_words, max_comment_length * embedding_dim)`\n","\n","model1.add(Flatten())\n","\n","# We add the classifier on top\n","model1.add(Dense(1, activation='sigmoid'))\n","\n","model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model1.summary()\n","\n","history = model1.fit(x_train, y_train,\n","                    epochs=20,\n","                    batch_size=32,\n","                    validation_data=(x_test, y_test))\n","\n","score1 = model1.evaluate(x_test, y_test)\n","\n","print(\"Accuracy: %.2f%%\" % (score1[1]*100))"]},{"cell_type":"markdown","metadata":{"id":"ghMCZRVB5HXo"},"source":["Utilizando una capa de embedding directamente obtenemos un 69%."]},{"cell_type":"markdown","metadata":{"id":"1z0t5kVY5HXp"},"source":["## Using pre-trained word embeddings\n","\n","\n","Sometimes, you have so little training data available that could never use your data alone to learn an appropriate task-specific embedding \n","of your vocabulary. What to do then?\n","\n","Instead of learning word embeddings jointly with the problem you want to solve, you could be loading embedding vectors from a pre-computed \n","embedding space known to be highly structured and to exhibit useful properties -- that captures generic aspects of language structure.\n","\n","Such word embeddings are generally computed using word occurrence statistics (observations about what words co-occur in sentences or \n","documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, low-dimensional embedding space \n","for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started really taking \n","off in research and industry applications after the release of one of the most famous and successful word embedding scheme: the Word2Vec \n","algorithm, developed by Mikolov at Google in 2013. Word2Vec dimensions capture specific semantic properties, e.g. gender.\n","\n","There are various pre-computed databases of word embeddings that can download and start using in a Keras `Embedding` layer. Word2Vec is one \n","of them. Another popular one is called \"GloVe\", developed by Stanford researchers in 2014. It stands for \"Global Vectors for Word \n","Representation\", and it is an embedding technique based on factorizing a matrix of word co-occurrence statistics. Its developers have made \n","available pre-computed embeddings for millions of English tokens, obtained from Wikipedia data or from Common Crawl data.\n","\n","Let's take a look at how you can get started using GloVe embeddings in a Keras model. The same method will of course be valid for Word2Vec \n","embeddings or any other word embedding database that you can download."]},{"cell_type":"markdown","metadata":{"id":"MzL5J7565HXr"},"source":["### Download the GloVe word embeddings\n","\n","\n","Head to `https://nlp.stanford.edu/projects/glove/` (where you can learn more about the GloVe algorithm), and download the pre-computed \n","embeddings from 2014 English Wikipedia + Gigaword 5. There are several word embedding files. We will use the small word embedding with 50 dimensions (glove.6B.50d)."]},{"cell_type":"markdown","metadata":{"id":"03QL2jZv5HXr"},"source":["### Pre-process the embeddings\n","\n","\n","Let's parse the un-zipped file (it's a `txt` file) to build an index mapping words (as strings) to their vector representation (as number \n","vectors)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEZv4hve5HXr","executionInfo":{"status":"ok","timestamp":1682186303379,"user_tz":-120,"elapsed":3789,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7684d5de-e8cd-4748-f9ea-ee5eb9a796f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 400000 word vectors.\n"]}],"source":["import os\n","import numpy as np\n","\n","glove_dir = '/content/drive/MyDrive/IA2/p3/'\n","\n","embeddings_index = {}\n","f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))"]},{"cell_type":"markdown","metadata":{"id":"vjPFiOfQ5HXr"},"source":["\n","Now let's build an embedding matrix that we will be able to load into an `Embedding` layer. It must be a matrix of shape `(max_words, \n","embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n","(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"m5Zs0sIU5HXs"},"outputs":[],"source":["embedding_dim = 50\n","\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if i < max_words:\n","        if embedding_vector is not None:\n","            # Words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector"]},{"cell_type":"markdown","metadata":{"id":"Zqvm9DNK5HXs"},"source":["### Define a model\n","\n","We will be using the same model architecture as before:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnP6jLBR5HXs","executionInfo":{"status":"ok","timestamp":1682186303380,"user_tz":-120,"elapsed":12,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"07971e98-a075-4b9e-a203-00a7e9b15181"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_4 (Embedding)     (None, 20, 50)            103550    \n","                                                                 \n"," flatten_4 (Flatten)         (None, 1000)              0         \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 1001      \n","                                                                 \n","=================================================================\n","Total params: 104,551\n","Trainable params: 104,551\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# MODELO 2. EMBEDDINGS PRE-ENTRENADOS CONGELADOS\n","\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model2 = Sequential()\n","model2.add(Embedding(max_words, embedding_dim, input_length=max_comment_length))\n","model2.add(Flatten())\n","model2.add(Dense(1, activation='sigmoid'))\n","model2.summary()"]},{"cell_type":"markdown","metadata":{"id":"Aq9tCKoe5HXs"},"source":["### Load the GloVe embeddings in the model\n","\n","\n","The `Embedding` layer has a single weight matrix: a 2D float matrix where each entry `i` is the word vector meant to be associated with \n","index `i`. Simple enough. Let's just load the GloVe matrix we prepared into our `Embedding` layer, the first layer in our model:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"x38tZ6Fn5HXs"},"outputs":[],"source":["model2.layers[0].set_weights([embedding_matrix])\n","model2.layers[0].trainable = False"]},{"cell_type":"markdown","metadata":{"id":"6KHAcFgG5HXt"},"source":["\n","Additionally, we freeze the embedding layer (we set its `trainable` attribute to `False`), then one part of a model is pre-trained (like our `Embedding` layer), and parts are randomly initialized (like our classifier), the pre-trained parts should not be updated during training to avoid forgetting what they already know. "]},{"cell_type":"markdown","metadata":{"id":"HcWGxxEU5HXt"},"source":["### Train and evaluate\n","\n","Let's compile our model and train it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMK_DYeI5HXt","executionInfo":{"status":"ok","timestamp":1682186306645,"user_tz":-120,"elapsed":3272,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"73c4b493-eb4d-44ec-9f57-e9bfc24cb742"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","25/25 [==============================] - 1s 8ms/step - loss: 0.7088 - accuracy: 0.5250 - val_loss: 0.6742 - val_accuracy: 0.6000\n","Epoch 2/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6420 - accuracy: 0.6100 - val_loss: 0.6552 - val_accuracy: 0.6200\n","Epoch 3/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.6047 - accuracy: 0.6675 - val_loss: 0.6434 - val_accuracy: 0.6300\n","Epoch 4/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.5752 - accuracy: 0.7225 - val_loss: 0.6359 - val_accuracy: 0.6300\n","Epoch 5/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.5514 - accuracy: 0.7475 - val_loss: 0.6313 - val_accuracy: 0.6550\n","Epoch 6/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.7613 - val_loss: 0.6282 - val_accuracy: 0.6650\n","Epoch 7/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.5142 - accuracy: 0.7763 - val_loss: 0.6228 - val_accuracy: 0.6750\n","Epoch 8/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.4994 - accuracy: 0.7937 - val_loss: 0.6191 - val_accuracy: 0.6900\n","Epoch 9/20\n","25/25 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7937 - val_loss: 0.6172 - val_accuracy: 0.6850\n","Epoch 10/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.4711 - accuracy: 0.8112 - val_loss: 0.6128 - val_accuracy: 0.6950\n","Epoch 11/20\n","25/25 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.8200 - val_loss: 0.6100 - val_accuracy: 0.6950\n","Epoch 12/20\n","25/25 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.8275 - val_loss: 0.6089 - val_accuracy: 0.6900\n","Epoch 13/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.8300 - val_loss: 0.6071 - val_accuracy: 0.6950\n","Epoch 14/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.8413 - val_loss: 0.6056 - val_accuracy: 0.7000\n","Epoch 15/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.4231 - accuracy: 0.8375 - val_loss: 0.6054 - val_accuracy: 0.7100\n","Epoch 16/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.4143 - accuracy: 0.8500 - val_loss: 0.6052 - val_accuracy: 0.6950\n","Epoch 17/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.4071 - accuracy: 0.8475 - val_loss: 0.6031 - val_accuracy: 0.7000\n","Epoch 18/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.3996 - accuracy: 0.8537 - val_loss: 0.6047 - val_accuracy: 0.7000\n","Epoch 19/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.3933 - accuracy: 0.8537 - val_loss: 0.6036 - val_accuracy: 0.7050\n","Epoch 20/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.3868 - accuracy: 0.8562 - val_loss: 0.6025 - val_accuracy: 0.7000\n","7/7 [==============================] - 0s 2ms/step - loss: 0.6025 - accuracy: 0.7000\n"]}],"source":["model2.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model2.fit(x_train, y_train,\n","                    epochs=20,\n","                    batch_size=32,\n","                    validation_data=(x_test, y_test))\n","\n","score2 = model2.evaluate(x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"03mPNZ2j5HXu"},"source":["\n","We obtain 70% accuracy with the pre-trained model. It's a better result because we don't have too much training data.\n"]},{"cell_type":"markdown","source":["\n","We can also try to train the same model without freezing the embedding layer. In that case, we would be adapting our word embedding learning a classification task.\n","\n","Let's try it:"],"metadata":{"id":"vIUu9Xed9qXE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtegIcSE5HXu","executionInfo":{"status":"ok","timestamp":1682186309967,"user_tz":-120,"elapsed":3332,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d410cbe9-6569-49be-b51e-03fdfcee06ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_5 (Embedding)     (None, 20, 50)            103550    \n","                                                                 \n"," flatten_5 (Flatten)         (None, 1000)              0         \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 1001      \n","                                                                 \n","=================================================================\n","Total params: 104,551\n","Trainable params: 104,551\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/20\n","25/25 [==============================] - 1s 9ms/step - loss: 0.7043 - accuracy: 0.5050 - val_loss: 0.6927 - val_accuracy: 0.6200\n","Epoch 2/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.6284 - accuracy: 0.6425 - val_loss: 0.6719 - val_accuracy: 0.6450\n","Epoch 3/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.7113 - val_loss: 0.6550 - val_accuracy: 0.6300\n","Epoch 4/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.5323 - accuracy: 0.7638 - val_loss: 0.6416 - val_accuracy: 0.6650\n","Epoch 5/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.4951 - accuracy: 0.7987 - val_loss: 0.6308 - val_accuracy: 0.6600\n","Epoch 6/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.4593 - accuracy: 0.8375 - val_loss: 0.6207 - val_accuracy: 0.6700\n","Epoch 7/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.4261 - accuracy: 0.8712 - val_loss: 0.6098 - val_accuracy: 0.6750\n","Epoch 8/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.3942 - accuracy: 0.8900 - val_loss: 0.6037 - val_accuracy: 0.7150\n","Epoch 9/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.3655 - accuracy: 0.9038 - val_loss: 0.5956 - val_accuracy: 0.6900\n","Epoch 10/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.3374 - accuracy: 0.9237 - val_loss: 0.5889 - val_accuracy: 0.7050\n","Epoch 11/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.3108 - accuracy: 0.9362 - val_loss: 0.5836 - val_accuracy: 0.7100\n","Epoch 12/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.2859 - accuracy: 0.9450 - val_loss: 0.5779 - val_accuracy: 0.7150\n","Epoch 13/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.2626 - accuracy: 0.9500 - val_loss: 0.5724 - val_accuracy: 0.7350\n","Epoch 14/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.2402 - accuracy: 0.9638 - val_loss: 0.5693 - val_accuracy: 0.7350\n","Epoch 15/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.2201 - accuracy: 0.9712 - val_loss: 0.5643 - val_accuracy: 0.7500\n","Epoch 16/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.2016 - accuracy: 0.9762 - val_loss: 0.5613 - val_accuracy: 0.7550\n","Epoch 17/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9787 - val_loss: 0.5602 - val_accuracy: 0.7450\n","Epoch 18/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.1701 - accuracy: 0.9812 - val_loss: 0.5585 - val_accuracy: 0.7550\n","Epoch 19/20\n","25/25 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9837 - val_loss: 0.5568 - val_accuracy: 0.7450\n","Epoch 20/20\n","25/25 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9850 - val_loss: 0.5558 - val_accuracy: 0.7500\n","7/7 [==============================] - 0s 2ms/step - loss: 0.5558 - accuracy: 0.7500\n"]}],"source":["# MODELO3. EMBEDDINGS PREENTRENADOS SIN CONGELAR\n","\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model3 = Sequential()\n","model3.add(Embedding(max_words, embedding_dim, input_length=max_comment_length))\n","model3.add(Flatten())\n","model3.add(Dense(1, activation='sigmoid'))\n","model3.summary()\n","\n","model3.layers[0].set_weights([embedding_matrix])\n","model3.layers[0].trainable = True\n","\n","model3.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","history = model3.fit(x_train, y_train,\n","                    epochs=20,\n","                    batch_size=32,\n","                    validation_data=(x_test, y_test))\n","\n","score3 = model3.evaluate(x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"jvppc0cR5HXu"},"source":["\n","We obtain 75% accuracy with the pre-trained model not freezed. In this case, it's the best configuration to our classification problem.\n"]},{"cell_type":"markdown","source":["# Resumen de resultados"],"metadata":{"id":"ycQc3CyjFNxI"}},{"cell_type":"code","source":["print(\"Sin word embeddings pre-entrenados\")\n","print(\"Accuracy: %.2f%%\" % (score1[1]*100))\n","print(\"Con word embeddings pre-entrenados congelados\")\n","print(\"Accuracy: %.2f%%\" % (score2[1]*100))\n","print(\"Con word embeddings pre-entrenados sin congelar\")\n","print(\"Accuracy: %.2f%%\" % (score3[1]*100))"],"metadata":{"id":"PG05YT6hssVI","executionInfo":{"status":"ok","timestamp":1682187099603,"user_tz":-120,"elapsed":200,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"96c17eec-0a13-413b-dab0-7ad8787a11a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sin word embeddings pre-entrenados\n","Accuracy: 69.00%\n","Con word embeddings pre-entrenados congelados\n","Accuracy: 70.00%\n","Con word embeddings pre-entrenados sin congelar\n","Accuracy: 75.00%\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}